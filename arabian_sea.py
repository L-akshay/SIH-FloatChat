# -*- coding: utf-8 -*-
"""Arabian Sea.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/199cphjT6OWFEXfgwEcE7IRjIbuo-m1R5
"""

pip install argopy #importing argopy library

from argopy import DataFetcher , set_options
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sklearn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error ,r2_score
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor

print("numpy",np.__version__)
print("pandas",pd.__version__)
print("sklearn",sklearn.__version__)

set_options(src='argovis')

#Arabian sea data
fetcher1 = DataFetcher().region([60,90,0,20,0,1000])
data1 = fetcher1.load().data

data1 #Arabian sea data in xArray format

type(data1)

#Converting xrray object to pandas Dataframe
df1 = data1.to_dataframe().reset_index()

df1.head()

df1.info()
#missing value : PSAL , TEMP

#Trajectory of floats in Arabian Sea
fetcher1.plot('trajectory')

df1.PSAL#salinity

#statistical insights for salinity recorded by folats in arabian sea
df1.PSAL.describe()

df1.isnull().sum()

df1.shape

df1.TEMP

#Dropping unecessary columns
df1 = df1.drop(columns=['N_POINTS','CYCLE_NUMBER','DATA_MODE','DIRECTION','PLATFORM_NUMBER','POSITION_QC','TIME_QC','TIME'])

df1.head()

#X : Input features ('LATITUDE','LONGITUDE','PRES','PSAL')
#y : Output feature(TEMP)
X = df1[['LATITUDE','LONGITUDE','PRES','PSAL']]
y = df1['TEMP']

X= X[~y.isna()]
y = y.dropna()

#Converting all the column names to string objects
X.columns = X.columns.map(lambda x : str(x))

X.head()

#Filling of missing values of salinity
trf1 = ColumnTransformer([
    ("imputer_PSAL",SimpleImputer(),[3]),
],remainder="passthrough")

#Decision tree Regression model
trf2 = DecisionTreeRegressor()

#Randdom forest Regression model
random_forest = RandomForestRegressor()

lr = LinearRegression()
from sklearn.svm import SVR
svm = SVR(kernel='rbf')

#Creation of pipeline of Simple Imputer (transfomer) and Decison Tree Regressor (estimator)
pipe = Pipeline([
    ('trf1',trf1),
    ('trf2',trf2),
])

#Creation of pipeline of Simple Imputer (transfomer) and Random Forest Regressor (estimator)
pipe2 = Pipeline([
    ('trf1',trf1),
    ('trf2',random_forest)

])

pipe3 = Pipeline([
    ('trf1',trf1),
    ('trf2',lr)
])

pipe4 = Pipeline([
    ('trf1',trf1),
    ('trf2',svm)
])

#performing train test split
#fitting training set on first pipeline
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
pipe.fit(X_train,y_train)

pipe2.fit(X_train,y_train) #fitting training data on second pipeline

pipe3.fit(X_train,y_train)

pipe4.fit(X_train,y_train)

#Prediction of first and second pipelines on test set
y_pred_1 = pipe.predict(X_test)
y_pred_2 = pipe2.predict(X_test)
y_pred_3 = pipe3.predict(X_test)
y_pred_4 = pipe4.predict(X_test)

r1 = r2_score(y_test,y_pred_1)
r2 = r2_score(y_test,y_pred_2)
r3 = r2_score(y_test,y_pred_3)
r4 = r2_score(y_test,y_pred_4)

#Evaluation of better model by comparing metrics
print("R2 score for Decision Tree",r2_score(y_test,y_pred_1))
print("R2 score for Random Forest ",r2_score(y_test,y_pred_2))
print("R2 score for Linear Regression ",r2_score(y_test,y_pred_3))
print("R2 score for Support Vector Machine",r2_score(y_test,y_pred_4))



import matplotlib.pyplot as plt
models = ["Decision Tree", "Random Forest", "Linear Regression", "Support Vector Machine"]
accuracies = [r1, r2, r3, r4]

# --- Bar Chart ---
plt.bar(models, accuracies)
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison")
plt.xticks(rotation=20)
plt.yticks(np.arange(0, 1.05, 0.05))
plt.show()

import matplotlib.pyplot as plt
import numpy as np

models = ["Decision Tree", "Random Forest", "Linear Regression", "Support Vector Machine"]
accuracies = [r1, r2, r3, r4]

# Convert to percentage
accuracies_percent = [a * 100 for a in accuracies]

# --- Bar Chart ---
plt.bar(models, accuracies_percent)
plt.ylabel("Accuracy (%)")
plt.title("Model Accuracy Comparison")
plt.xticks(rotation=20)

# Y-axis ticks (0 to 100 with step of 5)
plt.yticks(np.arange(0, 105, 5))

plt.show()

import matplotlib.pyplot as plt

models = ["Decision Tree", "Random Forest", "Linear Regression", "Support Vector Machine"]
accuracies = [r1, r2, r3, r4]

# --- Line Plot ---
plt.plot(models, accuracies, marker='o', linestyle='-', color='b')
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison")
plt.ylim(0, 1)  # keeps accuracy between 0 and 1
plt.grid(True)
plt.show()

print("RMSE score for Decision Tree",np.sqrt(mean_squared_error(y_test,y_pred_1)))
print("RMSE score for Random Forest ",np.sqrt(mean_squared_error(y_test,y_pred_2)))

#Exporting temp_1_model locally
#temp_1_model predicts temperature
import joblib
joblib.dump(pipe2, "temp_1_model.pkl")

from google.colab import files
files.download("temp_1_model.pkl")

df1.head()

#Changing input features to include temperature
#Target feature : salinity
X1 = df1[["LATITUDE","LONGITUDE","PRES",'TEMP']]
y1 = df1["PSAL"]

X1.columns = X1.columns.map(lambda x : str(x))

X1.head()

X1 = X1[~y1.isna()]
y1 = y1.dropna()

#Filling the missing values
trf2 = ColumnTransformer([
    ('imputer_TEMP',SimpleImputer(),[3])
],remainder="passthrough")

trf3 = DecisionTreeRegressor()

trf4 = RandomForestRegressor()

#Creation of two pipelines
#transformer used : SimpleImputer(fills missing values)
#pipe3 : estimator = Random Forest Regressor
#pipe4 : estimator = Decision Tree Regressor
pipe3 = Pipeline([
    ('trf1',trf2),
    ('trf2',trf3)
])

pipe4 = Pipeline([
    ('trf1',trf2),
    ('trf2',trf4)
])

#performing train test split
X1_train,X1_test,y1_train,y1_test = train_test_split(X1,y1,test_size=0.2,random_state=42)

#fitting the traiining data on pipe3 and pipe4 pipelines
pipe3.fit(X1_train,y1_train)

pipe4.fit(X1_train,y1_train)

#Prediction of both models on testing set
y_pred_1 = pipe3.predict(X1_test)
y_pred_2 = pipe4.predict(X1_test)

#Evaluating both models through metrics
print("R2 score for decision tree: ",r2_score(y1_test,y_pred_1))
print("R2 score for random forest: ",r2_score(y1_test,y_pred_2))

print("RMSE Score for decision tree: ",np.sqrt(mean_squared_error(y1_test,y_pred_1)))
print("RMSE Score for random forest: ",np.sqrt(mean_squared_error(y1_test,y_pred_2)))

#Exporting model locally
#psal_1_model predicts salinity
import joblib
joblib.dump(pipe4, "psal_1_model.pkl")

from google.colab import files
files.download("psal_1_model.pkl")

pip install huggingface_hub

from huggingface_hub import login
login()

from google.colab import drive
drive.mount('/content/drive')

#exporting psal_1_model to huggingface
from huggingface_hub import upload_file

repo_id = "Suyash1120/Arabian_Sea_argovis"  # replace with your repo
upload_file(
    path_or_fileobj=r"/content/psal_1_model.pkl",         # local file
    path_in_repo="psal_1_model.pkl",            # how it will appear online
    repo_id=repo_id,
    repo_type="model"
)

#exporting temp_1_model to huggingface
from huggingface_hub import upload_file

repo_id = "Suyash1120/Arabian_Sea_argovis"  # replace with your repo
upload_file(
    path_or_fileobj="/content/temp_1_model.pkl",         # local file
    path_in_repo="temp_1_model.pkl",            # how it will appear online
    repo_id=repo_id,
    repo_type="model"
)

from google.colab import drive
drive.mount('/content/drive')

